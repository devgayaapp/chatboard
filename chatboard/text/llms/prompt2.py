
import json
from typing import Any, List, Optional, Union
from langchain_core.pydantic_v1 import BaseModel, ConfigDict, Field

from chatboard.text.llms.conversation import SystemMessage, HumanMessage
from chatboard.text.llms.views import Action, ViewModel, Type
from .llm import AzureOpenAiLLM, OpenAiLLM
from .tracer import Tracer
import textwrap
import inspect
import asyncio
import itertools

def is_async_function(obj):
    return inspect.iscoroutinefunction(obj) or inspect.isasyncgenfunction(obj)


def get_func_args(func):
    return list(inspect.signature(func).parameters.keys())

def flatten_list(nested_list):
    flat_list = []
    for item in nested_list:
        if isinstance(item, list):
            flat_list.extend(flatten_list(item))
        else:
            flat_list.append(item)
    return flat_list




def filter_func_args(func, args):
    return {k: v for k, v in args.items() if k in get_func_args(func)}

class ChatPrompt(BaseModel):
    name: Optional[str] = None
    model: str= "gpt-3.5-turbo-0125"
    llm: Union[OpenAiLLM, AzureOpenAiLLM] = Field(default_factory=OpenAiLLM)
    is_traceable: bool=True
    tools: Optional[List[Type[Action]]] = None


    def __init__(self, **data):
        super().__init__(**data)
        if self.name is None:
            self.name = self.__class__.__name__

        # tool_fields = [field for field in self.__class__.__fields__.items() if isinstance(field[1].annotation, type) and issubclass(field[1].annotation, Action)]
        # self.tools = {tool_fields[1].annotation.__name__ : {
        #     "name": tool_fields[0],
        #     "info": tool_fields[1],            
        # } for tool_fields in tool_fields}
        self._build_tool_lookup()

    class Config:
        arbitrary_types_allowed = True

    def _build_tool_lookup(self):
        tool_fields = [field for field in self.__class__.__fields__.items() if isinstance(field[1].annotation, type) and issubclass(field[1].annotation, Action)]
        self.tools = {tool_fields[1].annotation.__name__ : {
            "name": tool_fields[0],
            "info": tool_fields[1],            
        } for tool_fields in tool_fields}


    async def complete(self):
        return []    


    async def _build_conversation(self, context=None, **kwargs: Any):
        conversation = []
        kwargs['context'] = context
        filtered_args = filter_func_args(self.complete, kwargs)
        completion_views = await self.complete(**filtered_args)
        # if 'context' in get_func_args(self.complete):
        #     completion_views = await self.complete(context=context, **kwargs)
        # else:
        #     completion_views = await self.complete(**kwargs)
        # for view in completion_views:
            # content_out = await view.render()
            # content = textwrap.dedent(content_out).strip()
            # if view.system:                
            #     conversation.append(SystemMessage(content=content))
            # else:
            #     conversation.append(HumanMessage(content=content))
        conversation = await asyncio.gather(*completion_views)        
        flat_conversation = flatten_list(conversation)
        return flat_conversation
        # return conversation


    async def preprocess(self, **kwargs: Any):
        return kwargs
    

    def convert_to_openai_tool(self):
        return [tool_cls["info"].annotation.to_tool() for tool_cls in self.tools.values()]

    async def __call__(self, prompt=None, context=None, tracer_run=None, output_conversation=False, **kwargs: Any) -> Any:
            if prompt is not None:
                kwargs['prompt'] = prompt
            if 'model' not in kwargs:
                kwargs["model"] = self.model
            log_kwargs = {}
            log_kwargs.update(kwargs)            
            
            if prompt is not None:
                log_kwargs['prompt'] = prompt
            with Tracer(
                is_traceable=self.is_traceable,
                tracer_run=tracer_run,
                name=self.name,
                run_type="prompt",
                inputs={
                    "input": log_kwargs,
                    # "messages": conversation.messages
                },
                # extra=extra,
            ) as prompt_run:                
                msgs = await self._build_conversation(context=context, **kwargs)
                msgs = [m for m in msgs if m is not None]

                tools = self.convert_to_openai_tool()
                if not tools:
                    tools = None

                completion_msg = await self.llm.complete(
                        msgs=msgs,
                        tools=tools,
                        tracer_run=prompt_run, 
                        **kwargs
                    )                                
                
                prompt_run.end(outputs={'output': completion_msg})
                if context:
                    context.history.add(
                            view_name=self.name, 
                            view_cls=self.__class__, 
                            inputs=log_kwargs, 
                            output=completion_msg, 
                            msgs=msgs
                        )
                if completion_msg.tool_calls is not None:
                    completion_msg = await self._handle_tool_call(context, completion_msg)
                    # for tool_call in completion_msg.tool_calls:
                    #     tool_args = json.loads(tool_call.function.arguments)                        
                    #     toll_cls = self.tools['UserDetails']['info'].default.__class__
                    #     tool = toll_cls(**tool_args)
                    #     _attr = getattr(self, self.tools[tool_call.function.name]["name"])
                    #     output = await _attr.handle(tool)
                    #     completion_msg = output

                return completion_msg
    
    async def _handle_tool_call(self, context, completion_msg):
        for tool_call in completion_msg.tool_calls:
            tool_args = json.loads(tool_call.function.arguments)
            tool_cls = self.tools[tool_call.function.name]['info'].default.__class__
            # tool = tool_cls(**tool_args)
            # _attr = getattr(self, self.tools[tool_call.function.name]["name"])
            # output = await _attr.handle(tool)
            output = await self.handle(context, tool_cls, tool_args)
            return output
        
    async def handle(self, context, tool_cls, tool_args):
        tool = tool_cls(**tool_args)
        tool_args['context'] = context
        kwargs = filter_func_args(self.complete, tool_args)
        return await tool.handle(**kwargs)
        # _attr = getattr(self, self.tools[tool_call.function.name]["name"])
        # output = await _attr.handle(tool)
        # return output